import os
import json
import fitz
from glob import glob
from langchain_ollama import ChatOllama
from langchain.schema import Document
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from langchain_community.tools import DuckDuckGoSearchRun
from datetime import datetime
from langgraph.graph import StateGraph, END
from typing import TypedDict, Optional
from langchain_core.tools import Tool

# from langchain_community.tools.tavily_search import TavilySearchResults
# from langchain_community.tools import SerperSearchRun

global_embed_model = SentenceTransformer('BAAI/bge-m3')
# global_embed_model = SentenceTransformer('intfloat/multilingual-e5-large', device='cpu')

# ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    query: str
    conversation_context: Optional[str]
    search_response: Optional[str]
    conversation_summary: Optional[str]
    final_answer: Optional[str]

class SearchAgent:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(SearchAgent, cls).__new__(cls)
            cls._instance.initialized = False
        return cls._instance

    def __init__(self):
        if self.initialized:
            return
        self.llm = ChatOllama(model="gemma3:12b-it-q4_K_M")
        # self.llm = ChatOllama(model='qwen3:14b-q4_K_M')
        # self.embed_model = SentenceTransformer('BAAI/bge-m3')
        self.embed_model = global_embed_model
        self.search = DuckDuckGoSearchRun()
        
        print("PDF ë¡œë”© ë° ì²­í¬ ìƒì„± ì¤‘...")
        raw_docs = self.extract_texts_from_pdfs("./data")
        self.chunks = self.split_text_to_chunks(raw_docs)
        embeddings = self.embed_model.encode([doc.page_content for doc in self.chunks])
        self.faiss_index = faiss.IndexFlatL2(embeddings[0].shape[0])
        self.faiss_index.add(np.array(embeddings))
        self.chunk_store = self.chunks
        self.initialized = True

        self.tools = [
            Tool(name="LocalSearch", func=self.faiss_search, description="FAISS ê¸°ë°˜ ë¡œì»¬ ë¬¸ì„œ ê²€ìƒ‰"),
            Tool(name="WebSearch", func=self.web_search_tool_func, description="DuckDuckGo ê¸°ë°˜ ì›¹ ê²€ìƒ‰")
        ]

    def extract_texts_from_pdfs(self, folder_path="./data"):
        pdf_paths = glob(os.path.join(folder_path, "*.pdf"))
        # pdf_paths = glob(os.path.join(folder_path, "*.txt"))
        pdf_paths.sort()
        all_texts = []
        for pdf_path in pdf_paths:
            print(pdf_path)
            doc = fitz.open(pdf_path)
            for page in doc:
                text = page.get_text()
                if text.strip():
                    all_texts.append(Document(page_content=text.strip()))
        return all_texts

    def split_text_to_chunks(self, docs, chunk_size=1000, chunk_overlap=100):
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=[".", "\n"]
        )
        return splitter.split_documents(docs)

    def faiss_search(self, query: str, threshold: float = 0.6, k: int = 7) -> str:
        q_emb = self.embed_model.encode([query])
        D, I = self.faiss_index.search(np.array(q_emb), k=k)
        
        results = []
        for dist, idx in zip(D[0], I[0]):
            if idx >= len(self.chunk_store):
                continue
            # L2 ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜ (0~1, ì‘ì„ìˆ˜ë¡ ìœ ì‚¬)
            similarity = 1 / (1 + dist)  # ê°„ë‹¨í•œ ì •ê·œí™” ë°©ì‹
            if similarity < threshold:
                continue  # ì‹ ë¢°ë„ ì ìˆ˜ê°€ ì„ê³„ê°’ ë¯¸ë§Œì´ë©´ ì œì™¸
            result = f"ë¬¸ì„œ {idx} (ì‹ ë¢°ë„: {similarity:.2f}):\n{self.chunk_store[idx].page_content}"
            results.append(result)
        
        if not results:
            print("Observation: No relevant documents found in local DB with sufficient confidence.")
            return f"ë¡œì»¬ DBì—ì„œ ì‹ ë¢°ë„ {threshold:.2f} ì´ìƒì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        print(f"Observation: Found {len(results)} relevant documents with confidence >= {threshold:.2f}.")
        return "\n\n".join(results)

    def web_search_tool_func(self, query: str) -> str:
        print("Action: Performing web search...")
        try:
            result = self.search.invoke(query)
            print(f"Observation: Web search result: {result[:100]}...")
            return result
        except Exception as e:
            print(f"Observation: Web search failed: {e}")
            return f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
            
    def process_query(self, state: AgentState) -> AgentState:
        query = state["query"]
        conversation_context = state.get("conversation_context", "")
        
        print(f"Thought: Processing query: {query}")
        
        local_result = self.faiss_search(query, threshold=0.57)  # ì‹ ë¢°ë„ ì„ê³„ê°’ ì¶”ê°€
        if "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤" in local_result:
            web_result = self.web_search_tool_func(query) # ë¡œì»¬ ê²°ê³¼ê°€ ì—†ì„ ê²½ìš°ì—ë§Œ ì›¹ ê²€ìƒ‰
        else:
            web_result = "ì›¹ ê²€ìƒ‰ ìƒëµ (ë¡œì»¬ì—ì„œ ì¶©ë¶„í•œ ì •ë³´ í™•ë³´)"

        prompt = f"""
        ë‹¹ì‹ ì€ ë‹¹ë‡¨ë³‘ ê´€ë¦¬ì— íŠ¹í™”ëœ AIì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— **í•œê¸€ë¡œë§Œ**, ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ê°„ê²°íˆ ë‹µë³€í•˜ì„¸ìš”.
        **ì§ˆë¬¸**: {query}
        **ë¡œì»¬ ê²€ìƒ‰ ê²°ê³¼**: {local_result[:500]}...
        **ì›¹ ê²€ìƒ‰ ê²°ê³¼**: {web_result[:500]}...
        **ëŒ€í™” ë¬¸ë§¥**: {conversation_context if conversation_context else "ë¬¸ë§¥ ì—†ìŒ"}
        **ì§€ì¹¨**:
        - ë¡œì»¬ ë¬¸ì„œë¥¼ ë¨¼ì € ì°¾ëŠ” ë° ê¼¼ê¼¼í•˜ê²Œ ì°¾ëŠ” ê²ƒì´ ì¢‹ìŒ.
        - ê° í•­ëª©ì€ í•œ ë¬¸ì¥ ì´ìƒ.
        - ëŒ€ì•ˆì„ ë¬´ì¡°ê±´ ì•Œë ¤ì¤„ ê²ƒ.
        - ì¶œì²˜ë¥¼ ê°„ë‹¨íˆ ë§ˆì§€ë§‰ì— ëª…ì‹œ.
        """
        try:
            print("Thought: Generating response with LLM...")
            response = self.llm.invoke(prompt)
            state["search_response"] = response.content
            print(f"Observation: Response generated: {state['search_response'][:100]}...")
        except Exception as e:
            print(f"Observation: LLM processing failed: {e}")
            state["search_response"] = f"ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"
        return state
    
class ConversationAgent:
    def __init__(self, memory_file="conversation_history.json"):
        self.llm = ChatOllama(model="gemma3:12b-it-q4_K_M")
        # self.llm = ChatOllama(model='qwen3:14b-q4_K_M')
        self.memory_file = memory_file
        self.conversation_history = self.load_conversation_history()
        # self.embed_model = SentenceTransformer('BAAI/bge-m3')
        self.embed_model = global_embed_model
        self.update_faiss_index()
        self.tools = [
            Tool(
                name="ConversationHistorySearch",
                func=self.search_conversation_history,
                description="ëŒ€í™” ê¸°ë¡ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."
            )
        ]

    def load_conversation_history(self):
        try:
            if os.path.exists(self.memory_file):
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return []
        except Exception as e:
            print(f"[ì˜¤ë¥˜] ëŒ€í™” ê¸°ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def save_conversation_history(self):
        try:
            with open(self.memory_file, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[ì˜¤ë¥˜] ëŒ€í™” ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")

    def update_faiss_index(self):
        history_texts = [f"Q: {entry['user_query']}\\nA: {entry['agent_response']}" 
                         for entry in self.conversation_history]
        if history_texts:
            embeddings = self.embed_model.encode(history_texts)
            self.faiss_index = faiss.IndexFlatL2(embeddings[0].shape[0])
            self.faiss_index.add(np.array(embeddings))
            self.history_store = history_texts
        else:
            self.faiss_index = None
            self.history_store = []

    def search_conversation_history(self, query: str) -> str:
        print("Action: Searching conversation history...")
        if not self.history_store or not self.faiss_index:
            print("Observation: No conversation history available.")
            return "ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
        q_emb = self.embed_model.encode([query])
        D, I = self.faiss_index.search(np.array(q_emb), k=5)
        results = [self.history_store[i] for i in I[0] if i < len(self.history_store)]
        if not results:
            print("Observation: No relevant conversation history found.")
            return "ê´€ë ¨ ëŒ€í™” ê¸°ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        print(f"Observation: Found {len(results)} relevant conversation entries.")
        return "\n\n".join(results)

    def update_conversation(self, state: AgentState) -> AgentState:
        query = state["query"]
        response = state["search_response"]
        entry = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "user_query": query,
            "agent_response": response,
            "source": "SearchAgent"
        }
        self.conversation_history.append(entry)
        self.save_conversation_history()
        self.update_faiss_index()
        state["conversation_context"] = self.get_conversation_context()
        return state

    def summarize_conversation(self, state: AgentState) -> AgentState:
        history_text = "\n".join([f"Q: {entry['user_query']}\nA: {entry['agent_response']}" 
                                  for entry in self.conversation_history[-5:]])  # ìµœê·¼ 5ê°œë¡œ ì œí•œ
        prompt = f"""
        ë‹¹ì‹ ì€ ëŒ€í™” ê¸°ë¡ì„ ìš”ì•½í•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ëŒ€í™”ë¥¼ **ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´**ë¡œ **í•œê¸€ë¡œ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•´ì„œ** ê°„ê²°íˆ ìš”ì•½í•´ ì£¼ì„¸ìš”.  
        ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.
        **ëŒ€í™” ë‚´ìš©**:  
        {history_text if history_text else "ëŒ€í™” ê¸°ë¡ ì—†ìŒ"}
        **ìš”ì•½**:
        """
        try:
            print("Thought: Generating conversation summary...")
            history_result = self.search_conversation_history(state["query"])
            prompt += f"\n**ì¶”ê°€ ëŒ€í™” ê¸°ë¡ ê²€ìƒ‰ ê²°ê³¼**:\n{history_result[:300]}..."  # ê²€ìƒ‰ ê²°ê³¼ ì œí•œ
            summary = self.llm.invoke(prompt)
            state["conversation_summary"] = summary.content
            print(f"Observation: Summary generated: {state['conversation_summary'][:100]}...")
        except Exception as e:
            print(f"Observation: Conversation summary failed: {e}")
            state["conversation_summary"] = f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}"
        return state

    def get_conversation_context(self):
        return "\n".join([f"Q: {entry['user_query']}\nA: {entry['agent_response']}" 
                          for entry in self.conversation_history[-3:]])
    
def router_node(state: AgentState) -> AgentState:
    return state

def router_condition(state: AgentState) -> str:
    summary_keywords = ["ìš”ì•½", "ì •ë¦¬", "ë¦¬ë·°", "ëŒ€í™” ë‚´ìš©", "ì§€ê¸ˆê¹Œì§€", "ì´ì „ ëŒ€í™”"]
    query = state["query"].lower()
    if any(keyword in query for keyword in summary_keywords):
        print("Thought: Detected summary intent in query.")
        return "conversation_agent_summarize"
    print("Thought: Defaulting to search intent.")
    return "search_agent"

def search_agent_node(state: AgentState) -> AgentState:
    search_agent = SearchAgent()  # ì‹±ê¸€í†¤ìœ¼ë¡œ í•œ ë²ˆë§Œ ì´ˆê¸°í™”
    return search_agent.process_query(state)

def conversation_agent_update_node(state: AgentState) -> AgentState:
    conversation_agent = ConversationAgent()
    return conversation_agent.update_conversation(state)

def conversation_agent_summarize_node(state: AgentState) -> AgentState:
    conversation_agent = ConversationAgent()
    return conversation_agent.summarize_conversation(state)

def combine_node(state: AgentState) -> AgentState:
    final = []
    if state.get("conversation_summary"):
        final.append(f"ëŒ€í™” ìš”ì•½:\n{state['conversation_summary']}")
    if state.get("search_response"):
        final.append(f"ê²€ìƒ‰ ê²°ê³¼:\n{state['search_response']}")
    state["final_answer"] = "\n\n".join(final)
    return state

graph = StateGraph(AgentState)
graph.add_node("router", router_node)
graph.add_node("search_agent", search_agent_node)
graph.add_node("conversation_agent_update", conversation_agent_update_node)
graph.add_node("conversation_agent_summarize", conversation_agent_summarize_node)
graph.add_node("combine", combine_node)

graph.set_entry_point("router")
graph.add_conditional_edges(
    "router",
    router_condition,
    {
        "search_agent": "search_agent",
        "conversation_agent_summarize": "conversation_agent_summarize"
    }
)
graph.add_edge("search_agent", "conversation_agent_update")
graph.add_edge("conversation_agent_update", "combine")
graph.add_edge("conversation_agent_summarize", "combine")
graph.set_finish_point("combine")

runnable_graph = graph.compile()

while True:
    user_input = input("\nğŸ§‘ ì‚¬ìš©ì ì§ˆë¬¸: ")
    if user_input.strip().lower() in ["exit", "quit"]:
        print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    try:
        input_state = {
            "query": user_input,
            "conversation_context": None,
            "search_response": None,
            "conversation_summary": None,
            "final_answer": None
        }
        result = runnable_graph.invoke(input_state, {"debug": True})
        print(f"\nğŸ¤– ì—ì´ì „íŠ¸ ì‘ë‹µ:\n{result['final_answer']}")
    except Exception as e:
        print(f"[ì˜¤ë¥˜ ë°œìƒ] {e}")